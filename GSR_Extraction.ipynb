{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSR_Extraction.ipynb",
      "provenance": [],
      "mount_file_id": "1TI2T7VLdddve_erUGxpKWOCOUJ8QFrXH",
      "authorship_tag": "ABX9TyNA1A1C7SvK82hQrDU9zvMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silvererudite/ML_algos_onSomeDatasets/blob/master/GSR_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvZrgm38ZfnA",
        "outputId": "856a2cb3-26d0-4914-e739-94ecbc15a94d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install neurokit2\n",
        "!pip install pysiology\n",
        "\n",
        "#some libraries are not used\n",
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "from statistics import mean\n",
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "from statistics import mean\n",
        "from scipy.signal import welch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n",
        "import neurokit2 as nk\n",
        "import pysiology as pl\n",
        "\n",
        "labels = pd.DataFrame()\n",
        "for m in range(1,33):\n",
        "    fl = f'/content/drive/My Drive/Deap Dataset/Copy of s{m}.dat'\n",
        "    with open(fl, 'rb') as fl:content = pickle.load(fl, encoding='latin1')\n",
        "    y=content[\"labels\"]\n",
        "    y=pd.DataFrame(y, columns=[\"valence\", \"arousal\",\"dominance\",\"likings\"])\n",
        "    y=y.iloc[:,0:4]\n",
        "    labels=pd.concat([labels, y], ignore_index=True)\n",
        "\n",
        "labels['Valence'] = labels['valence'].apply(lambda x: \"0\" if x <= 5 else \"1\")\n",
        "labels['Arousal'] = labels['arousal'].apply(lambda x: \"0\" if x <= 5 else \"1\")\n",
        "labels['Dominance'] = labels['dominance'].apply(lambda x: \"0\" if x <= 5 else \"1\")\n",
        "labels['Likings'] = labels['likings'].apply(lambda x: \"0\" if x <= 5 else \"1\")\n",
        "\n",
        "labels=labels.iloc[:,4:8]\n",
        "labels.shape\n",
        "\n",
        "labels.head\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: neurokit2 in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from neurokit2) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurokit2) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (0.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->neurokit2) (1.15.0)\n",
            "Requirement already satisfied: pysiology in /usr/local/lib/python3.7/dist-packages (0.0.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pysiology) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pysiology) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pysiology) (1.19.5)\n",
            "Requirement already satisfied: peakutils in /usr/local/lib/python3.7/dist-packages (from pysiology) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pysiology) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pysiology) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pysiology) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pysiology) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->pysiology) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of      Valence Arousal Dominance Likings\n",
              "0          1       1         1       1\n",
              "1          1       1         1       1\n",
              "2          1       1         1       1\n",
              "3          0       1         1       1\n",
              "4          1       0         1       1\n",
              "...      ...     ...       ...     ...\n",
              "1275       0       1         1       0\n",
              "1276       0       1         1       0\n",
              "1277       0       1         1       0\n",
              "1278       0       1         0       0\n",
              "1279       1       0         1       0\n",
              "\n",
              "[1280 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrGC4K0WWbRA",
        "outputId": "df3f2437-0a12-4907-93fc-5b0a950f8483"
      },
      "source": [
        "features = pd.DataFrame()\n",
        "channel_no=[36] #selecting channels\n",
        "for m in range(1,33):\n",
        "\n",
        "  print(m,end=\",\")\n",
        "\n",
        "  fl = f'/content/drive/My Drive/Deap Dataset/Copy of s{m}.dat'\n",
        "\n",
        "  with open(fl, 'rb') as fl: \n",
        "    content = pickle.load(fl, encoding='latin1')\n",
        "\n",
        "  for i in range(0,40): #videos\n",
        "    \n",
        "    x=content[\"data\"][i]\n",
        "    feature_list=[]\n",
        "    df2=pd.DataFrame()\n",
        "\n",
        "    for j in channel_no: #channels\n",
        "\n",
        "      gsr= x[j]\n",
        "\n",
        "      x = pl.electrodermalactivity.analyzeGSR(gsr.tolist(), samplerate=128, preprocessing=False)\n",
        "\n",
        "      check_list=['peak', 'riseTime', 'latency', 'amplitude', 'halfAmplitude', 'halfAmplitudeIndex', 'halfAmplitudeIndexPre', 'EDAatApex', 'decayTime', 'SCRWitdth']\n",
        "\n",
        "      for key, val in x.items():\n",
        "\n",
        "        if len(check_list) == len(val.keys()):\n",
        "\n",
        "          feature_list.append(x[key]['peak']['peakEnd'])\n",
        "          feature_list.append(x[key]['peakMax'] ['peakMax'])\n",
        "          feature_list.append(x[key]['peakStart']['peakStart'])\n",
        "\n",
        "          feature_list.append(x[key]['EDAatApex'])\n",
        "          feature_list.append(x[key]['SCRWitdth'])\n",
        "          feature_list.append(x[key]['halfAmplitudeIndexPre'])\n",
        "          feature_list.append(x[key]['decayTime'])\n",
        "\n",
        "          feature_list.append(x[key]['amplitude'])\n",
        "          feature_list.append(x[key]['halfAmplitude'])\n",
        "          feature_list.append(x[key]['halfAmplitudeIndex'])\n",
        "          feature_list.append(x[key]['latency'])\n",
        "          feature_list.append(x[key]['riseTime'])\n",
        "          break\n",
        "\n",
        "\n",
        "    d_f1 = pd.DataFrame(feature_list)\n",
        "    df=d_f1.to_numpy()\n",
        "    data=np.reshape(df,(1,-1))\n",
        "    df2=pd.concat([df2,pd.DataFrame(data=data)],ignore_index=True)\n",
        "\n",
        "    features=pd.concat([features, df2],  ignore_index=True)\n",
        "\n",
        "#print(feature_list)\n",
        "Features= pd.concat([features, labels], axis=1, sort=False)\n",
        "Features = Features.dropna(how='any',axis=0)\n",
        "\n",
        "val=Features.iloc[:,:]\n",
        "#val.rename(columns={ val.columns[:,0:11]: check_list }, inplace = True)\n",
        "val.to_csv(r'feature_GSR.csv', index = False, header=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYUqaLIaH84"
      },
      "source": [
        "# features = pd.DataFrame()\n",
        "\n",
        "# channel_no=[38] #selecting channels\n",
        "\n",
        "# for m in range(1,33):\n",
        "\n",
        "#   print(m,end=\",\")\n",
        "\n",
        "#   fl = f'/content/drive/My Drive/Deap Dataset/Copy of s{m}.dat'\n",
        "\n",
        "#   with open(fl, 'rb') as fl: content = pickle.load(fl, encoding='latin1')\n",
        "\n",
        "#   for i in range(0,40): #videos\n",
        "    \n",
        "#     x=content[\"data\"][i]\n",
        "\n",
        "#     df2=pd.DataFrame()\n",
        "\n",
        "#     for j in channel_no: #channels\n",
        "\n",
        "#       ppg= x[j]  \n",
        "          \n",
        "#       ppg_cleaned = nk.ppg_clean(ppg_signal= ppg, sampling_rate=128)\n",
        "\n",
        "#       # Find peaks\n",
        "#       peaks = nk.ppg_findpeaks(ppg_cleaned, sampling_rate=128)\n",
        "\n",
        "#       Feature_list = nk.hrv_nonlinear(peaks, sampling_rate=128, show=False)\n",
        "#       print(\"Features ppg\", Feature_list)\n",
        "\n",
        "\n",
        "\n",
        "#     d_f1 = pd.DataFrame(Feature_list)\n",
        "#     df=d_f1.to_numpy()\n",
        "#     data=np.reshape(df,(1,-1))\n",
        "#     df2=pd.concat([df2,pd.DataFrame(data=data)],ignore_index=True)\n",
        "\n",
        "#     features=pd.concat([features, df2], ignore_index=True)\n",
        "\n",
        "# features.shape\n",
        "\n",
        "# Features= pd.concat([features, labels], axis=1, sort=False)\n",
        "# Features = Features.dropna(how='any',axis=0)\n",
        "# #Features\n",
        "# #print(\"Features ppg\", feature_list)\n",
        "# #val=Features.iloc[:,:]\n",
        "# #val.to_csv(r'feature_ppg.csv', index = False, header=True)\n"
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}